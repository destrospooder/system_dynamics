# An Engineering Student's Butchering of Ma326

## Preface {-}

This chapter's going to be a smorgasbord of mathematical concepts I think are foundational to understanding this course from a theoretical perspective. A decent chunk of it will be review from Ma110 and Ma240, but it doesn't hurt to take a second look at these things (they keep coming back over and over). That being said, this chapter largely strays off from Prof. Luchtenburg's curriculum, especially in the latter half of section 4.3.



What I'll try to do, instead of rehashing what you got out of Ma240, is reframe these concepts in a way that's more applicable to this course (and ME351). I'll also formalize a bunch of concepts from Ma326 that are really important to know for this class (and later on). But first, let's formalize a few more definitions that will come in handy later on.




A **linear combination** of elements of a set $x_1, x_2, ..., x_n$, is given by:


$$a_1 x_1 + a_2 x_2 + ... + a_n x_n $$

where each $a_i$ is a constant. In layman's terms, a linear combination of variables is the sum of scaled versions of those variables, where the scaling factor is a scalar.



A set of variables are **linearly dependent** if one of the variables can be expressed as a linear combination of the others. More formally, a set of variables $x_1, x_2, ..., x_n$ is linearly dependent if there exist scalars $a_1, a_2, ... a_n$ (not all zero), such that:


$$a_1 x_1 + a_2 x_2 + ... + a_n x_n = 0$$

On the other hand, if no such scalars exist, the set of variables is said to be **linearly independent**. 

## Linearity and Time-Invariance

A \textbf{linear time-invariant (LTI) system} is a mathematical model often used in control theory to describe the behavior of physical systems. It is characterized by two properties: linearity and time-invariance. I'll describe these separately in the context of differential equations. A differential equation of the form:


$$a_n y^{(n)} (t) + a_{n-1} y^{(n-1)} + ... + a_1 \dot{y}(t) + a_0 y(t) = f(t)$$

where $y^{(i)}$ is the $i$th derivative of $y(t)$, is called \textbf{linear}. (The relationship between $y(t)$ and $t$ is a linear mapping.) A system is linear if and only if it satisfies two properties: superposition and homogeneity:

* **Superposition** - if $x_1 \to y_1$ and $x_2 \to y_2$, then $x_1 + x_2 \to y_1 + y_2$
* **Homogeneity** - if $k$ is a scalar and $x \to y$, then $kx \to ky$

If $\{a_i\}_0^n$, also called coefficients, are constants, the equation is also characterized as a \textbf{constant coefficient} differential equation. Physically, constant coefficients imply that the system behavior does not depend on time. 



We can establish an equivalence between linear constant coefficient equations and linear time-invariant systems. Time-invariance is the principle that if we chug an input $t_0$ into a system that outputs $y(t_0)$, the input $t_0 + t_1$ will result in an output of $y(t_0 + t_1)$.^[An equivalent definition in signal processing is that a system is time-invariant if it commutes with a "delay".]

Let's take a look at a few examples to make this more clear:


$$\dot{y} + \sin(t) y = 0$$

This system is not LTI, because the coefficient of $y$ is not constant. (More explicitly, it's a function of $t$, so as $t \to \infty$, the behavior is affected.)


$$2 \dot{y} + 3 y = 0$$

This system *is* LTI, because the coefficients of each derivative of $y$ are constant.


$$\dot{y} + \ln(y) = 0$$

This system isn't even linear, for obvious reasons.



Examples of LTI systems include first-order passive filters, second order systems such as springs and masses, and many other linear systems in control theory and signal processing.

## Matrices, at Lightspeed

I truly hope you know what a matrix is by now. If not, fasten your seatbelt.



A matrix is a rectangular array of numbers, of the form:


$$\textbf{A} = \begin{bmatrix}
    a_{11} & a_{12} & ... & a_{1n}\\ a_{21} & a_{22} & ... & a_{2n}\\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & ... & a_{mn}
\end{bmatrix}$$

where $a_{ij}$ is the entry at row $i$ and column $j$. We say a matrix is of size $m \times n$, where $m$ is the number of rows and $n$ is the number of columns.


$$\underbrace{\begin{bmatrix}
    1 & 2 \\ 3 & 4 \\ 5 & 6
\end{bmatrix}
}_{3\times 2}$$

Matrices are pretty neat. We can add numbers in the matrix elementwise and scale it by a scalar factor as follows:


$$\begin{bmatrix}
    a_1 & b_1 \\ c_1 & d_1
\end{bmatrix} + \begin{bmatrix}
    a_2 & b_2 \\ c_2 & d_2
\end{bmatrix} = \begin{bmatrix}
    a_1 + a_2 & b_1 + b_2 \\ c_1 + c_2 & d_1 + d_2
\end{bmatrix}$$

$$ k \begin{bmatrix}
    a & b \\ c & d
\end{bmatrix} = \begin{bmatrix}
    ka & kb \\ kc & kd
\end{bmatrix}$$

The **zero matrix** **0** is an $m\times n$ matrix with all entries 0. The zero matrix plays an important role in linear algebra, as it is the additive identity for matrices. This means that adding a zero matrix to any matrix does not change the matrix, much like adding zero to any number does not change its value.



A **square matrix** is a matrix with the same number of rows and columns. Many concepts in linear algebra are designed with these in mind, such as determinants and eigenvalues.



Next, we'll tackle the **Kronecker delta**, which is  is a useful and important symbol in mathematics, particularly in linear algebra and related fields. Its simple definition allows for the easy expression of many concepts and operations, making it a valuable tool for mathematicians and scientists. We define the Kronecker delta $\delta_{ij}$ as follows:


\begin{equation*}
    \delta_{ij} = \left\{ \begin{array}{ll}
        1 \quad i=j\\
        0 \quad i \neq j
    \end{array} \right.
\end{equation*}

Following from this, the **identity matrix** $\textbf{I}_n$ is defined as an $n\times n$ square matrix where $(\textbf{I}_n)_{ij} = \delta_{ij}$, i.e.,


$$\textbf{I}_1 = 1, \; \textbf{I}_2 = \begin{bmatrix}
    1 & 0 \\ 0 & 1
\end{bmatrix}, \; \textbf{I}_3 = \begin{bmatrix}
    1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
\end{bmatrix}, \; ...$$

The set of all matrices fixed at size $m \times n$ with scalar entries forms something we call a vector space. There's a lot of nuance attached to that name; if you care about it, take Ma326. Here are a few properties for now. (Bolded quantities are matrices, $a, b,$ and 1 are scalars.)

* Matrix addition is commutative, i.e., $\textbf{X} + \textbf{Y} = \textbf{Y} + \textbf{X}$
* Matrix addition is associative, i.e., $(\textbf{X} + \textbf{Y}) + \textbf{Z} = \textbf{X} + (\textbf{Y} + \textbf{Z})$
* Each matrix has an additive identity, i.e., $\textbf{X} + \textbf{0} = \textbf{X}$
* Each matrix has an additive inverse, i.e., $\textbf{X} + \textbf{Y} = \textbf{0}$
* Each matrix has a scalar multiplicative identity, i.e., $1 (\textbf{X}) = \textbf{X}$
* $(ab)\textbf{X} = a(b\textbf{X})$
* $a(\textbf{X} + \textbf{Y}) = a\textbf{X} + a\textbf{Y}$
* $(a+b)\textbf{X} = a\textbf{X} + b\textbf{X}$

We can multiply two matrices of sizes $m\times n$ and $n\times p$, respectively, to produce another matrix of size $m\times p$. The operation, dubbed **matrix multiplication**, should not be confused with the scalar multiplication used before. It's defined as follows:


$$(\textbf{AB})_{ij} = \sum_{k=1}^n \textbf{A}_{ik} \textbf{B}_{kj}$$

Finding the product of two matrices may look intimidating based off that formula, but it's actually not that difficult once you understand the basic steps. First, you need to make sure that the matrices are compatible for multiplication. To do this, we need to ensure that the number of columns in the first matrix is the same as the number of rows in the second matrix. If they're not the same, you can't multiply them.^[This is a really great gut check when you're finishing up a long calculation. If that matrix multiplication at the end of the problem is impossible, something must be up.]



Once you've determined that the matrices are compatible, you can start multiplying. To find each element in the product matrix, you need to multiply the corresponding row in the first matrix by the corresponding column in the second matrix. Specifically, for each element in the product matrix, you will:

* Take the row of the first matrix that corresponds to that element.
* Take the column of the second matrix that corresponds to that element.
* Multiply each corresponding pair of elements in the row and column.
* Add up all of the products you got in the last step.

Keep doing this for every element in the product matrix until you've filled in all the entries. Here's a brief example:

$$\begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6
    \end{bmatrix}
    \begin{bmatrix}
    1 & 2 \\
    3 & 4 \\
    5 & 6
    \end{bmatrix}
    =
    \begin{bmatrix}
    1 \cdot 1 + 2 \cdot 3 + 3 \cdot 5 & 1 \cdot 2 + 2 \cdot 4 + 3 \cdot 6 \\
    4 \cdot 1 + 5 \cdot 3 + 6 \cdot 5 & 4 \cdot 2 + 5 \cdot 4 + 6 \cdot 6\end{bmatrix} = \begin{bmatrix} 22 & 28 \\ 49 & 64 \end{bmatrix}$$



Matrix products show up everywhere. They're essential for fields like population modeling, network theory, signal processing, advanced dynamics, etc. Try to be as comfortable as possible with this operation before diving into the next chapter.

## Transposition and Symmetry

The **transpose** of an $m\times n$ matrix $\textbf{A}$ is the $n\times m$ matrix obtained by interchanging its rows and columns. It's often denoted as $\textbf{A}^\text{T}$ or $\textbf{A}^\text{t}$.^[I prefer the former notation and will be using it henceforth.] In other words, the rows of the original matrix become columns in the transposed matrix, and the columns become rows.


$$\textbf{A} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix} \qquad \qquad \textbf{A}^\text{T} = \begin{bmatrix} 1 & 3 & 5 \\ 2 & 4 & 6 \end{bmatrix}$$

Let's follow up with a few more properties that involve transposition. (Bolded quantities are matrices, $a, b$ are scalars.)

* $\left(\textbf{X}^\text{T}\right)^\text{T} = \textbf{X}$
* $(\textbf{X}+\textbf{Y})^\text{T} = \textbf{X}^\text{T} + \textbf{B}^\text{T}$
* $a\textbf{X}^\text{T} = (a\textbf{X})^\text{T}$
* $(\textbf{X}\textbf{Y})^\text{T} = \textbf{Y}^\text{T} \textbf{X}^\text{T}$

A **symmetric matrix** is a square matrix equal to its own transpose. (Another way to phrase this is: if $\textbf{A}$ is an $n\times n$ matrix, then $\textbf{A}$ is symmetric if and only if $\textbf{A}^\text{T} = \textbf{A}$.)

Trivially, the sum of two symmetric matrices is symmetric, and a scalar multiple of a symmetric matrix is symmetric.

There's other kinds of symmetry as well. For example, a **skew-symmetric matrix** is a square matrix equal to its negative. (Another way to phrase this is: if $\textbf{A}$ is an $n\times n$ matrix, then $\textbf{A}$ is skew-symmetric if and only if $\textbf{A}^\text{T} = -\textbf{A}$.) This turns out to be *much* more interesting than its vanilla counterpart in dynamics, especially when dealing with concepts like inertial tensors and angular momentum.



Again, trivially, the sum of two skew-symmetric matrices is symmetric, and a scalar multiple of a skew-symmetric matrix is symmetric. Additionally, you may realize that every entry on the diagonal of a skew-symmetric matrix must be equal to 0. (Otherwise, how would the definition work?) 



This factoid turns out to be crucial if we focus on the 3-space case, there are only three independent entries of a skew-symmetric matrix. We can define a skew-symmetric operator for vectors in 3-space skew() as follows:^[Yeah, you'll see awful notation everywhere for this thing. I'm going to use skew() because why not.]


$$\text{skew}(\boldsymbol{x}) = \text{skew}(x_1, x_2, x_3) = \begin{bmatrix} 0 & -x_1 & x_2 \\ x_1 & 0 & -x_3 \\ -x_2 & x_3 & 0\end{bmatrix} $$

This operator comes with a really cool property:


$$\text{skew}(\boldsymbol{x})^\text{T} = -\text{skew}(\boldsymbol{x})$$

which comes in handy a lot in advanced dynamics. Additionally, we can make an alternative definition of the vector cross product.


$$\boldsymbol{x} \times \boldsymbol{y} = \text{skew}(\boldsymbol{x}) \boldsymbol{y}$$

Prove it! It's kind of fun.

## Invert It! Determine...It!
