# A Mishmash of Modeling Concepts

## Preface - Time Domain Modeling {-}

When we begin to analyze more complicated systems, it becomes less and less feasible to solve them analytically. As a result, we resort to setting up a system of differential equations rather than concatenating them into one as we've done in past chapters.



The state-space representation of a system describes the system's behavior over time in terms of a set of variables called **states**.^[The state space can be described as a Euclidean space where each state corresponds with an axis.] The state variables represent the current conditions of the system, and their evolution over time is described by a set of first order differential equations called **state equations**.



The state-space representation is a very powerful tool for modeling and analyzing physical systems, providing valuable insights into their behavior and enabling the development of control algorithms covered in ME351.



An important thing to note is that the state-space representation of a system is not unique. In fact, an infinite number of representations exist for a physical system.

## The State-Space Approach

In the most general case, a state-state representation can be represented as the following:

\begin{equation*}
    \left\{ \begin{array}{ll}
        \underline{\dot{x}} = \underline{f}(\underline{x}, \underline{u}) \\
        \underline{y} = \underline{h}(\underline{x}, \underline{u})
    \end{array} \right\.
\end{equation*}

This might be a bit daunting at first, but it's just a lot of fancy notation for a concept that's pretty simple. $\underline{x}$ is the state, $\underline{u}$ is an input, and $\underline{y}$ is an output. Let's drive this concept home with an example.



Say we have a simple pendulum with length $L$ and a point mass $m$ at its end, as pictured below:

For this problem, the mass of the rod (and any potential friction in the hinge) is ignored. The equation of motion of the pendulum can be derived by summing moments about the point of contact between the pendulum and the fixed surface.^[Alternatively, you could sum forces in the parallel and perpendicular directions of motion to yield an equivalent result.] Let's call that point of contact $O$ for future bookkeeping purposes.

$$\sum{M_O} = J_O \ddot{\theta}$$

The moment arm for the weight $mg$ is the horizontal displacement $L \sin(\theta)$, and $J_O = mL^2$ is the mass moment of inertia of the point mass $m$ about point $O$. Let's crunch some numbers.

$$-mgL \sin(\theta) = mL^2 \ddot{\theta}$$
$$mL^2 \ddot{\theta} + mgL \sin(\theta) = 0$$
$$\ddot{\theta} + \frac{g}{L} \sin(\theta) = 0$$

Now let's try putting this in state-space form. First, we select the state vector, which should adhere to the following points:

* Pick state variables that include all the relevant information about the system you're trying to model.
* The number of dimensions in the state vector should match the number of degrees of freedom of the system.
* The state vector should be **minimal**, meaning it should contain only the information necessary to describe the system, and not any redundant information. (Usually, the minimum number required is equal to the order of the differential equation that represents the system.)
* The components of the state vector must be linearly independent.

A good rule of thumb is that the state should correspond with the initial conditions provided. We define states $\theta$ (angle) and $\omega = \dot{\theta}$ (angular velocity), and start constructing our state equations.

$$\underline{x} = \begin{bmatrix}
    \theta \\
    \omega  \end{bmatrix} \qquad \qquad \underline{\dot{x}} = \begin{bmatrix}
        \dot{\theta} \\
        \dot{\omega}  \end{bmatrix}$$
$$\underline{\dot{x}} = \underline{f} (\underline{x}, \underline{u})$$

$$\dot{\underline{x}} = \begin{bmatrix}
    \dot{\theta} \\
    \dot{\omega}  \end{bmatrix} = \begin{bmatrix}
    \omega \\
    -\frac{g}{L} \sin(\theta) 
\end{bmatrix}$$

We've turned a second order differential equation into two first order differential equations. Let's move onto the second part of the representation: defining the output $y$. We're interested in the states' behavior over time, so our output is...just the state vector $\underline{x}$.

$$\underline{y} = \underline{h}(\underline{x}, \underline{u}) = \underline{x} = \begin{bmatrix}
    \theta \\
    \omega  \end{bmatrix}$$

These two components make up the state-space representation of this pendulum system. Putting it in this form makes it easier to numerically solve using tools like Python or MATLAB.



A nonlinear solution can be unappealing, though perfectly valid. By using the small-angle approximation $\sin(\theta) \sim \theta$, we can refine this representation further.

$$\ddot{\theta} + \frac{g}{L} \sin(\theta) \sim \ddot{\theta} + \frac{g}{L} \theta = 0$$

Our system is now an LTI system. Linearity is very nice, because we can use matrix multiplication to make this representation pretty.

$$\dot{\underline{x}} = \begin{bmatrix}
    \dot{\theta} \\
    \dot{\omega}  \end{bmatrix} = \begin{bmatrix}
    \omega \\
    -\frac{g}{L} \theta
\end{bmatrix} = \begin{bmatrix}
    0 & 1\\
    -\frac{g}{L} & 0
\end{bmatrix} \begin{bmatrix}
    \theta \\
    \omega  \end{bmatrix}$$

Let's move onto the **output equation**, which is (in my opinion) less interesting than the state equation. What are we interested in analyzing here? Say we're interested in analyzing $\theta$ - or more succinctly, $y = \theta$.

$$\underline{y} = \begin{bmatrix}
    \theta \\
    \omega  \end{bmatrix} = \begin{bmatrix}
        1 & 0
    \end{bmatrix} \begin{bmatrix}
        \theta \\
        \omega  \end{bmatrix}$$

Or, more interestingly, say we want to track both states over time ($\theta$ and $\omega$). Our output is just the state, so we set $y=\underline{x}$. In these cases, the "coefficient" matrix is the $n\times n$ identity matrix $\textbf{I}_n$, where $n$ is the number of components in the state vector.

$$\underline{y} = \begin{bmatrix}
    \theta \\
    \omega  \end{bmatrix} = \begin{bmatrix}
        1 & 0\\
        0 & 1
    \end{bmatrix} \begin{bmatrix}
        \theta \\
        \omega  \end{bmatrix} = \textbf{I}_2 \begin{bmatrix}
            \theta \\
            \omega  \end{bmatrix}$$

We'll move on using this output equation. Let's make this more complicated by saying the pendulum has an input applied torque $T$, and the new equation of motion is:

$$\ddot{\theta} + \frac{g}{L} \theta = \frac{T}{mL^2}$$

Our revised state-space representation would be:

$$\dot{\underline{x}} = \begin{bmatrix}
    \dot{\theta} \\
    \dot{\omega}  \end{bmatrix} = \begin{bmatrix}
    \omega \\
    -\frac{g}{L} \theta + \frac{T}{mL^2}
\end{bmatrix} = \begin{bmatrix}
    0 & 1\\
    -\frac{g}{L} & 0
\end{bmatrix} \begin{bmatrix}
    \theta \\
    \omega  \end{bmatrix} + \begin{bmatrix}
        0 \\
        \frac{1}{mL^2}  \end{bmatrix} T = \begin{bmatrix}
            0 & 1\\
            -\frac{g}{L} & 0
        \end{bmatrix} \underline{x} + \begin{bmatrix}
                0 \\
                \frac{1}{mL^2}  \end{bmatrix} u$$
$$\underline{y} = \begin{bmatrix}
    \theta \\
    \omega  \end{bmatrix} = \begin{bmatrix}
        1 & 0\\
        0 & 1
    \end{bmatrix} \begin{bmatrix}
        \theta \\
        \omega  \end{bmatrix} + \begin{bmatrix}
            0 \\
            0  \end{bmatrix} T = \begin{bmatrix}
                1 & 0\\
                0 & 1
            \end{bmatrix} \underline{x} + \begin{bmatrix}
                    0 \\
                    0  \end{bmatrix} u$$

Using this example, we now extract the general form of the state-space representation of a linear system. A linear system is represented in state space by the following equations:

$$\underline{\dot{x}} = \textbf{A} \underline{x} + \textbf{B} \underline{u}$$


$$\underline{y} = \textbf{C} \underline{x} + \textbf{D} \underline{u}$$

for $t \ge t_0$, $\underline{x}(t_0)$, where:

* $\underline{x}$ is the state vector, of size $n \times 1$
* $\underline{y}$ is the output vector, of size $q \times 1$
* $\underline{u}$ is the input vector, of size $p \times 1$
* $\textbf{A}$ is the system matrix, of size $n \times n$
* $\textbf{B}$ is the input matrix, of size $n \times p$
* $\textbf{C}$ is the output matrix, of size $q \times n$
* $\textbf{D}$ is the feedforward matrix, of size $q \times p$

Let's try another example, this time a translational mechanical system. Block 1 of mass $m$ is attached to a fixed wall by dashpot with damping coefficient $b$. Block 2, also of mass $m$, is attached to block 1 by a spring of spring constant $k$. Gravity is turned off.

First, we write the equations of motion of the network. (Just draw a free body diagram around each mass and don't fuck up your signs.)



> You want a hint? Newton guy.
>
> Prof. Luchtenburg

$$m \ddot{q_1} + b \dot{q_1} + k q_1 - k q_2 = 0$$
$$-kq_1 + m \ddot{q_2} + k q_2 = f(t)$$

This is a system of two second order differential equations, so we'll pick four states. We select our $q_1$, $\dot{q_1}$, $q_2$, and $\dot{q_2}$ to be our four state variables, because we're analyzing the kinematic behavior of two masses obeying Newton's second law (N2L is a second order differential equation, which requires two initial conditions, and since there's two masses to analyze we have four).



Our first two state equations are easy: just define the derivatives. We get the other two by rearranging the equations of motion and isolating $\ddot{q_1}$ and $\ddot{q_2}$.

$$\dot{\underline{x}} = \begin{bmatrix}
    \dot{q_1} \\
    \ddot{q_1} \\
    \dot{q_2} \\ 
    \ddot{q_2}
  \end{bmatrix} = \begin{bmatrix}
    0 & 1 & 0 & 0\\
    -\frac{k}{m} & -\frac{b}{m} & \frac{k}{m} & 0 \\
    0 & 0 & 0 & 1\\
    \frac{k}{m} & 0 & -\frac{k}{m} & 0
\end{bmatrix} \begin{bmatrix}
    q_1 \\
    \dot{q_1} \\
    q_2 \\ 
    \dot{q_2} \end{bmatrix} + \begin{bmatrix}
        0 \\
    0 \\
    0 \\ 
    \frac{1}{m} \end{bmatrix} f(t)$$

Now, we didn't specify what output we wanted, but let's say we want to analyze the velocity of the second mass, or $\dot{q_2}$. We'd do the following:

$$y = \begin{bmatrix} 0 & 0 & 0 & 1\end{bmatrix} \begin{bmatrix}
    q_1 \\
    \dot{q_1} \\
    q_2 \\ 
    \dot{q_2} \end{bmatrix} + \begin{bmatrix} 0 \end{bmatrix} f(t)$$

which is equivalent to the expression $y = \dot{q_2}$. Even though it seems more complicated to put it in this form, it provides us with a $\textbf{C}$ and $\textbf{D}$ matrix, which is invaluable when analyzing systems computationally.



What makes this example more significant than the previous one is that there's now two different "entities" to analyze.^[You could try to put the two tanks problem from HW1 in state-space form for extra practice.] Rather than just analyzing multiple states (position, velocity) of a singular entity like we did with the pendulum, we're now taking a look at the position and velocity of *two* masses. That's kind of nifty, I think.

## The Unit Impulse

*THWACK!*



You hear that? It's the sound of Prof. Baglione smacking something with an impact hammer, a tool used to simultaneously excite something and measure the impact force at the same time.



It's really common in mechanical (and electrical, ugh) scenarios to want to analyze a very large force over a very short period of time. Say we're interested in modeling a batter's hit, a car crash, or...I don't know...whacking a mass-spring system with an impact hammer.



Consider a model of a hammer strike, pictured as follows.

This strike is modeled as a rectangular "pulse", where the rule is:

$$F(t) = \begin{cases}
    0 & \text{if } t \leq t_0 - \epsilon \\
    \frac{F_0}{2\epsilon} & \text{if } t_0 - \epsilon < t < t_0 + \epsilon \\
    0 & \text{if } t \ge t_0 + \epsilon
\end{cases}$$

When integrated over time, $F(t)$ yields the **impulse** over $F$.

$$\text{Imp} = \int_{0^-}^{\infty} F(t) \, dt = \int_{t_0 - \epsilon}^{t_0 + \epsilon} \frac{F_0}{2\epsilon} \, dt = \frac{F_0}{2\epsilon} \left((t_0 + \epsilon)-(t_0 - \epsilon)\right) = F_0$$

This trick of "ripping out the integrand" only works when $\epsilon \neq 0$. Nevertheless, we investigate the case where $\epsilon \to 0$.^[Figure from Zill, et al. (8)]

As $\epsilon$ gets smaller and smaller, the magnitude of force $\frac{F_0}{2\epsilon}$ gets larger and larger. Nevertheless, the impulse remains equal to $F_0$. We define the **impulse function** as a function $F(t)$ with the following two properties:

$$F(t-t_0)=0, \; \; t\neq t_0 \qquad \qquad \int_{0^-}^\infty F(t) \, dt = F_0$$

We call an impulse function where $F_0 = 1$ the **unit impulse function**, also called the **Dirac delta function**, denoted $\delta(t)$.^[Not to be confused with the Kronecker delta!] Let's rewrite the above principles for the unit impulse.

$$\delta(t-t_0)=0, \; \; t\neq t_0 \qquad \qquad \int_{0^-}^\infty \delta(t) \, dt = 1$$

It's important to realize that the unit impulse function...well...isn't a function. The idea of a "function" that is equal to 0 everywhere except $t=t_0$, where it's equal to $\infty$ is preposterous! Additionally, for this "function" to make sense, the integral would be equal to 0.



**We don't care. We defined it that way.**^[The less dismissive answer is that the unit impulse function is an example of something we call a generalized function. Go take a functional analysis course if you want to pursue this further.]



There exists an alternative definition of the unit impulse used in signal processing (that will also end up useful later on in this course).

$$\int_{0^-}^\infty \delta(t-t_0) f(t)\, dt = f(t_0)$$

where $f$ is a continuous function. This is called the **sifting property** of the unit impulse function. The unit impulse function acts as a sampler; the juicy part of $\delta(t-t_0)$ is located at $t=t_0$, so integrating the product of the impulse $\delta(t-t_0)$ and a continuous function $f(t)$ yields $f(t_0)$.

## Preface - Frequency Domain Prerequisites {-}

While using the state-space representation of a system can be advantageous in situations where we have multiple inputs and multiple outputs, sometimes the linear algebra just gets too unwieldy, especially if you don't have Python or MATLAB sitting in front of you. (This isn't to say that transfer functions aren't easily implementable in Python or MATLAB, though.)



In lieu of modeling in the time domain, we can use **transfer functions** to mathematically model systems in the frequency domain. This ends up being really useful in ME351 when we talk about how to control physical systems rather than just analyzing them. Transfer functions are also much easier to translate into graphical interpretations of systems, like Bode plots.

## The Laplace Transform

You've likely brushed upon Laplace transforms in Ma240, so I'll try to reintroduce them...err...less formally.



The Laplace transform is an essential tool for analyzing and designing control systems, making them a vital topic to cover in preparation for ME351. By learning about Laplace transforms, we can develop a deep understanding of the behavior of dynamic systems and how to control them.



Laplace transforms allow us to simplify differential equations that describe the behavior of a system in the time-domain, into simpler algebraic equations in the $s$-domain. These equations can be easily analyzed to determine important system characteristics such as stability, steady-state error, and transient response.



Furthermore, understanding Laplace transforms is crucial for designing controllers that can effectively control the behavior of a system. For example, by using Laplace transforms to analyze a system's frequency response, engineers can design controllers that attenuate unwanted frequencies, leading to more desirable system performance.



Let's cut to the chase. The (one-sided) Laplace transform of a function $f(t)$ is a new function $F(s)$, $s \in \mathbb{C}$, defined by:^[The two-sided Laplace transform instead has integral limits of $-\infty$ and $\infty$, but for causal signal inputs, the one-sided Laplace transform is equivalent. (A causal signal is a signal that is 0 for all $t<0$.)]
$$ \mathcal{L} \{ f(t) \} = F(s) = \int_{0^-} ^\infty e^{-st} f(t) dt $$

As the Laplace transform is an integral transform, we know that it is linear. Thus, taking the Laplace transform of a linear combination of functions yields a linear combination of the Laplace transforms of the functions:

$$\mathcal{L} \{ \alpha f(t) + \beta g(t) \} = \mathcal{L} \{ \alpha f(t) \} + \mathcal{L} \{ \beta g(t) \} = \alpha \mathcal{L} \{ f(t) \} + \beta \mathcal{L} \{ g(t) \} = \alpha F(s) + \beta G(s)$$

where $\alpha, \,\beta$ are constants. Here are a few common ones (easily verifiable with the integral definition).

$$\mathcal{L} \{ \delta(t) \} = 1$$
$$\mathcal{L} \{ 1 \} = \frac{1}{s}$$
$$\mathcal{L} \{ e^{at} \} = \frac{1}{s-a}$$

Confirm them yourself! (Or find a table of Laplace transforms, I don't know...)



Our objective is to use the Laplace transform to model physical systems. Thus, it is imperative that we find a way to take the Laplace transform of a derivative. Provided $f$ and its derivatives are continuous from $0 \le t < \infty$, and are of exponential order,^[A function $f(t)$ is said to be of exponential order if there exist positive constants $a$ and $b$ such that $|f(t)| \le ae^{bt}$ for all $t\ge 0$.] we do so as follows:

$$\mathcal{L} \left \{\frac{df(t)}{dt} \right \} = \int_{0^-} ^\infty e^{-st} \frac{df(t)}{dt} dt = \left[ e^{-st} f(t) \right]_{0^-}^{\infty} - \int_{0^-} ^\infty -s e^{-st} f(t) dt = -f(0) + sF(s)$$

or:

$$\mathcal{L} \left \{ \frac{df(t)}{dt} \right \} = sF(s) - f(0)$$

By recursion, it is apparent that the Laplace transform of a second derivative is:

$$\mathcal{L} \left \{\frac{d^2f(t)}{dt^2} \right \} = sF'(s) - f'(0)= s(sF(s) - f(0)) - f'(0) = s^2 F(s) - sf(0) - f'(0)$$

and an $n$th derivative is:

$$\mathcal{L} \left \{\frac{d^n f(t)}{dt^n} \right \} = s^n F(s) - s^{n-1} f(0) - s^{n-2} f'(0) - ...- s f^{(n-2)}(0) - f^{(n-1)} (0)$$

In the case that all initial conditions are zero, it's valid to write:

$$\mathcal{L} \left\{\frac{d^n f(t)}{dt^n}\right\} = s^n F(s)$$

This will be handy very soon.



Finally, we'll introduce the notion of the **inverse Laplace transform**, which transforms an expression in the $s$-domain back into the time domain.^[It might seem peculiar that I'm abstaining from providing a formula to directly calculate the inverse Laplace transform. You can look up Mellin's inverse formula for additional hardship, but rest assured that it's WAY more convenient to think of the inverse Laplace transform as just that - an inverse Laplace transform, instead of as its own distinct transform.] More concisely:

$$\mathcal{L}^{-1} \left\{F(s)\right\} = f(t)$$

The inverse Laplace transform is also a linear transformation. Notably, it's not unique; there exist distinct $f$ and $g$ such that $\mathcal{L} \left\{f \right\} = \mathcal{L} \left\{g \right\}$.



Using the technology we've formalized^[Man, I said I was going to do this less formally!] thus far, we can use the Laplace transform to solve initial value problems. Say we wanted to solve the IVP:

$$\ddot{y} - 4\dot{y} + 3 y = e^{2t}$$

where $y$ is a function of $t$, and $y(0) = \dot{y}(0) = 0$. First, we transform the whole equation into the $s$-domain.

$$s^2 Y - 4 sY + 3 Y = \frac{1}{s-2}$$

Since our objective is to solve for $y(t)$, it's wise to isolate $Y(s)$ on one side, so when we take the inverse Laplace transform, we immediately have an explicit formula $y$.

$$Y (s^2 - 4 s + 3) = \frac{1}{s-2}$$
$$Y = \frac{1}{(s-2)(s^2 - 4 s + 3)}$$

We then find the partial fraction decomposition of this rational function, so that transforming back to the time domain is easier. ^[If you're not that comfortable with partial fraction decomposition, it might be a good idea to look up the Heaviside cover-up method. Thank me later.]

$$Y = -\left(\frac{1}{s-2} \right) + \frac{1}{2}\left(\frac{1}{s-1} \right) + \frac{1}{2}\left(\frac{1}{s-3}\right)$$

And finally, we transform back.

$$y = -e^{2t} + \frac{1}{2} e^{t} + \frac{1}{2} e^{3t}$$

which matches whatever we'd get if we tried using a time domain method like the method of undetermined coefficients. Of course, we did a problem with zero initial conditions, but rest assured, if your goal is just to solve differential equations, we can consider cases with nonzero initial conditions as well.

## It's Convoluted!

But what if we wanted to find the Laplace transform of an *integral*?



**Convolution** is an operation that generates a function from two other piecewise continuous functions. We define the convolution $f \ast g$ of functions $f(t)$ and $g(t)$ as follows:

$$f(t) \ast g(t) = \int_0^t f(\tau) g(t-\tau) \, d\tau = \int_0^t f(t-\tau) g(\tau) \, d\tau$$

So what makes this operation special? The Laplace transform of the convolution $f(t) \ast g(t)$ is equal to the product of the Laplace transform of $f(t)$ and $g(t)$.

$$\mathcal{L} \left\{f\ast g \right\} = \mathcal{L} \left\{f(t) \right\} \mathcal{L} \left\{g(t) \right\} = F(s) G(s)$$

Or alternatively:

$$\mathcal{L}^{-1} \left\{F(s) G(s) \right\} = f \ast g$$

So in the case that $g(t) = 1$, the Laplace transform of the integral of $f(t)$ is:

$$\mathcal{L} \left\{\int_0^t f(\tau) d\tau \right\} = \frac{F(s)}{s}$$

## Transfer Functions and Block Diagrams

When possible, we try to think of all systems as black boxes with an input and an output. We've covered how to represent a system in the time domain using the state-space representation. Let's try something else.



**Transfer functions** are often preferred over state space representations in certain situations because they are simple and easy to work with, especially for linear systems with a single input and a single output. They can be easily manipulated using some algebra and can be used to compute a system's response to various input signals. 



Transfer functions are also very useful for frequency-domain analysis, such as calculating the system's frequency response or designing filters to tweak the system's frequency characteristics. 



Mathematically, a transfer function is the ratio of the Laplace transform of the output of a system $Y(s)$ to the Laplace transform of its input $U(s)$, with all initial conditions assumed to be zero. The transfer function is typically denoted as $H(s)$, where:

$$H(s) = \frac{Y(s)}{U(s)}$$

That's all there is to it. Let's try a variation of the system we tackled before.

$$\ddot{y} - 4 \dot{y} + 3y = u$$

So instead of that exponential expression on the right side of the equation, we now have an arbitrary input $u(t)$. Because we're trying to find the transfer function, we assume zero initial conditions and take the Laplace transform of everything.

$$s^2 Y - 4 s Y + 3Y = U$$

Then, we find $H$.

$$H = \frac{Y}{U} = \frac{1}{s^2 - 4s + 3}$$

Using this transfer function, we can find the system response to any input we want. Say we're interested in the response to a step input $u_s(t)$. We can retrofit the definition of a transfer function to solve for the system response.

$$Y = H U$$

In this scenario, $U$ is the Laplace transform of $u_s(t)$, or $\frac{1}{s}$.

$$Y = \frac{1}{s^3 - 4s^2 + 3s}$$

which is left to complete as an exercise to the reader.^[I'm too lazy to take the inverse Laplace transform of this. It's not difficult though, knock yourself out.]



Ah, one more thing! The roots of the numerator of the transfer function are called the **zeroes** of the system, and the roots of the denominator of the transfer function are called the **poles**. These definitions are consistent with the definitions we established earlier in the course.



This seems like as good a time as any to introduce the concept of a **block diagram**, or a visual representation of a system, using blocks and arrows to break down components. We literally represent components as black boxes, without regard for whatever's going on inside. All that matters is what comes in and what goes out. Seems perfect for a transfer function.

Here's a block diagram. $U$ is our input signal, $Y$ is the output signal, and $H$ is the transfer function representation of the system. When a signal enters a system in a block diagram, the output signal will be the product of the signal and the system's transfer function.



We can use additional tools of block diagram algebra to simplify and manipulate these diagrams, which ends up being vital when we start introducing feedback control into the mix.

## Electrical Networks and Impedance

In ESC221, you may have covered the series RLC circuit, pictured as follows:

where the source voltage $V_s$ is the input of the system and the voltage across the capacitor $V_c$ is the output. The governing equation of this system is a second order differential equation, but using transfer functions, we can turn this differential equation into an algebraic one.



This is especially advantageous when conducting an alternating current (AC) analysis of the network, when $V_s$ could be a sinusoidal function. However, the formulation described in this course is applicable to any input signal.



The **impedance** $Z$ of a two-terminal passive component like a resistor, capacitor, or inductor, is defined as the ratio of the Laplace transform of the voltage $\tilde{V}$ to the Laplace transform of the current $\tilde{I}$.



Ohm's law states that $V = IR$. When we take the Laplace transform of the expression, we get:

$$\tilde{V} = \tilde{I} R$$

Thus, the impedance of a resistor is:

$$Z_R = \frac{\tilde{V}}{\tilde{I}} = R$$

That was...anticlimactic. Let's try a capacitor next. The capacitive relationship states that $q = \int I \, dt = CV$. When we take the Laplace transform of the expression, we get:

$$\frac{1}{s} \tilde{I} = C\tilde{V}$$

Thus, the impedance of a capacitor is:

$$Z_C = \frac{\tilde{V}}{\tilde{I}} = \frac{1}{sC}$$

That's more interesting. Finally, the inductive relationship states that $V = L\frac{dI}{dt}$. Again, Laplace transform:

$$\tilde{V} = s L\tilde{I}$$

and the impedance of an inductor is:

$$Z_L = \frac{\tilde{V}}{\tilde{I}} = sL$$

Here's a bit more context - think of impedance as the frequency domain "value" of a passive component. Complex resistance, if you will. Resistance is real, so there's no reason impedance is different in the Laplace'd analog of Ohm's law.

Relatedly, for a direct current (DC) circuit, $s=0$.



Let's take a look at the circuit again. The governing equation, found using Kirchhoff's voltage law, is:

$$V_s - RI - L \frac{dI}{dt} - V_c = 0$$

Using these newfound impedances, we can immediately convert this system to a transfer function. Let's Laplace the shit out of this thing. (Keep in mind that a transfer function is output over input.)

$$\tilde{V_s} - \tilde{I} (Z_R + Z_L + Z_C) = 0$$
$$\tilde{V_s} - \tilde{I} (R + sL + \frac{1}{sC}) = 0$$
$$U = \tilde{V_s} = \frac{\tilde{I}}{R + sL + \frac{1}{sC}}\qquad \qquad Y = \tilde{V_c} = \frac{\tilde{I}}{sC}$$
$$\frac{Y}{U} = \frac{\frac{\tilde{I}}{sC}}{\frac{\tilde{I}}{R + sL + \frac{1}{sC}}} = \frac{s^2 CL + sCR + 1}{s^2 C^2}$$

We can expand this idea to more than just passive components.^[In the electrical networks world, we can translate operational amplifiers (op amps) into impedances as well. Absolutely terrifying.] Impedance is a reliable method for thinking about mechanical systems as well (such as mass-spring systems, systems involving gears, motors, etc.)



The pigeonhole here is that these systems are modeled as linear. The frequency domain does not translate well when we study nonlinear systems; instead, we tend to use more qualitative methods such as drawing phase portraits or bifurcation analysis.

## Intermezzo - Discretization {-}

Let's take a look at a first order continuous system.

$$\tau \dot{y} + y = Ku$$

We can discretize this system by using finite differences in lieu of derivatives. At time $t_k$, we can approximate the derivative $\dot{y}(t_k)$ as follows:

$$\dot{y}(t_k) \approx \frac{y(t_k) - y(t_{k-1})}{t_k - t_{k-1}}$$

Plugging in, we get:

$$\tau \frac{y(t_k) - y(t_{k-1})}{t_k - t_{k-1}} + y(t_{k-1}) = Ku(t_{k-1})$$

Let's define $\Delta t = t_k - t_{k-1}$ and $\alpha = \frac{\tau}{\Delta t}$ With a bit more algebra, we find that:

$$y(t_k) = \frac{\alpha - 1}{\alpha} y(t_{k-1}) + \frac{K}{\alpha} u(t_{k-1})$$

One more simplification: $\beta = \alpha^{-1}$:

$$y(t_k) = (1-\beta) y(t_{k-1}) + K \beta u(t_{k-1})$$

Discretization makes it much easier to use computational tools like MATLAB and Python to manipulate a signal without preexisting toolboxes.

\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}

## Impulse Response

Now that we've developed all this new technology, let's draw some conclusions.



First, let's circle back to our discussion of the unit impulse. We define the **impulse response** $h(t)$ as a function that describes the behavior of an LTI system when it's stimulated with a unit impulse. The impulse response is the output of the system resulting from this very short, narrow input signal.



Say we smack a damped mass-spring system with an impact hammer, exciting the mass with a force we'll model as the unit impulse $\delta(t)$. 

The equation of motion of this system is:

$$m\ddot{x} + b \dot{x} + kx = \delta(t) \qquad \qquad \text{or} \qquad \qquad \ddot{x} + 2\zeta \omega_n \dot{x} + \omega_n^2 x = \frac{1}{m} \delta(t)$$

That's extremely daunting to solve, but we'll use a nifty trick. We separate the equation of motion into two..."stages" - during the hammer thwack and after the hammer thwack. Assume the mass is at rest before we whack it.



For the hammer thwack itself, we integrate the equation of motion twice successively from $0$ to time $t$, where $t$ is immediately after the smack:

$$\int_{0^-}^t \left(m \ddot{x} + b \dot{x} + kx = \delta(\tau) \right) \, d\tau$$
$$m \dot{x} + bx + \int_{0^-}^t kx \, d\tau = 1 \qquad \qquad \longleftrightarrow \qquad \qquad m \dot{x} = 1 - bx - \int_{0^-}^t kx \, d\tau$$

$$\int_{0^-}^t \left(\int_{0^-}^t \left(m \ddot{x} + b \dot{x} + kx = \delta(\tau) \right) \, d\tau \right) \,d\tau$$

$$m x + \int_{0^-}^t bx \, d\tau + \int_{0^-}^t 1 - \left(\int_{0^-}^t kx \, d\tau \right) \, d\tau = 0 \qquad \qquad \longleftrightarrow \qquad \qquad m x = \int_{0^-}^t 1 - bx - \left(\int_{0^-}^t kx \, d\tau \right) \, d\tau$$

Evaluating these integrals at $t=0^+$ yields the following two "initial conditions":
$$x(0^+) = 0 \qquad \qquad \dot{x}(0^+) = \frac{1}{m}$$

Since after the hammer crack there is no force exciting the system, solving the homogeneous version of the equation of motion with these initial conditions will yield the impulse response.

$$m\ddot{x} + b\dot{x} + kx = 0 , \; x(0^+) = 0 , \;\dot{x}(0^+) = \frac{1}{m}$$
$$x(t) = \frac{1}{m \omega_d} e^{-\zeta \omega_n t} \sin(\omega_d t) = h(t)$$



You'll get much more familiar with the concept of mechanical impulse in ME301.



We can calculate the response of a system to an arbitrary force of varying magnitude using the impulse response. To calculate the response of a structure to an external force, we use the concept of superposition, which involves breaking down the force into small parts and adding up the response to each part to get the response.



For example, if we have a force that varies over time, we can divide it into small time intervals of length $\Delta t$, and at each interval $t_k$, we can calculate the response of the system to an impulse of force $\Delta F_k = F(t_k)\Delta t$.



To do this, we use the impulse response function $h(t)$, which represents the response of the system to a unit impulse of force applied at time $t=0$. Then, the response of the system to the impulse of force $\Delta F_k$ applied at time $t_k$ is given by $h(t-t_k)\Delta F_k$, where $h(t-t_k)$ is the impulse response function shifted in time by $t_k$.



Thus, the response of the system to the external force $F(t)$ is given by the sum of the responses to all the impulses:

$$y(t) = \sum_{k=1}^n \Delta F_k h(t-t_k) = \sum_{k=1}^n  F(t_k) h(t-t_k)\Delta t$$

By using smaller and smaller time intervals $\Delta t$, we can make this sum approach an integral:

$$y(t) = \int_0^t  F(\tau) h(t-\tau) \, d\tau$$

which you may recognize as the convolution integral. While we used a mechanical framework to develop intuition for this concept, we can generalize further by saying that **if you have the impulse response $h(t)$ of the system, you can determine the response to an arbitrary input $u(t)$** as follows:

$$y(t) = \int_0^t u(\tau) h(t-\tau) \, d\tau$$


Let's close out with one more thought. What if we have an input signal $U(s) = 1$? Then the output signal is just equal to the transfer function $H(s)$. Thus, it's valid to say that **the impulse response of a system is equivalent to the inverse Laplace transform of the transfer function**.^[This might be more apparent if you think about this outside the Laplace domain, where $y(t) = \delta(t)$.]

We've established that we have two main representations of a linear system: the state-space representation and a transfer function. What if we want to convert between them?



Well, it's pretty easy in Python/MATLAB - just use a command. What's 



It's possible to come up with infinitely many state-space representations of a system, but the transfer function of a system is unique.
